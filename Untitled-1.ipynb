{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy-6660.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy-2406.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy-5369.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy-4077.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy-3718.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28268</th>\n",
       "      <td>Suprise-1266.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28269</th>\n",
       "      <td>Suprise-1272.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28270</th>\n",
       "      <td>Suprise-252.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28271</th>\n",
       "      <td>Suprise-534.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28272</th>\n",
       "      <td>Suprise-1514.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28273 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image_id  labels\n",
       "0        Happy-6660.jpg       2\n",
       "1        Happy-2406.jpg       2\n",
       "2        Happy-5369.jpg       2\n",
       "3        Happy-4077.jpg       2\n",
       "4        Happy-3718.jpg       2\n",
       "...                 ...     ...\n",
       "28268  Suprise-1266.jpg       5\n",
       "28269  Suprise-1272.jpg       5\n",
       "28270   Suprise-252.jpg       5\n",
       "28271   Suprise-534.jpg       5\n",
       "28272  Suprise-1514.jpg       5\n",
       "\n",
       "[28273 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "folder_dir = '/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training'\n",
    "\n",
    "image_ids = []\n",
    "emotions = []\n",
    "\n",
    "for i in os.listdir(folder_dir):\n",
    "    if i != '.DS_Store':\n",
    "        emotion_folder_path = f'{folder_dir}/{i}'\n",
    "        for j in os.listdir(emotion_folder_path):\n",
    "            emotion = os.path.basename(emotion_folder_path)\n",
    "            image_ids.append(j)\n",
    "            emotions.append(emotion)\n",
    "\n",
    "d = {'image_id':image_ids,'emotions':emotions}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "df['labels'] =encoder.fit_transform(df['emotions']) \n",
    "\n",
    "df.drop('emotions',axis=1,inplace=True)\n",
    "\n",
    "display(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "2    7215\n",
      "3    4965\n",
      "4    4830\n",
      "1    4097\n",
      "0    3995\n",
      "5    3171\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.labels.value_counts())\n",
    "\n",
    "decoder = {'Happy':2,'Neutral':3,'Sad':4,'Fear':1,'Angry':0,'Surprise':5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class Images(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Load in the dataset and assign to a dataframe\n",
    "        self.df = pd.read_csv('training_data.csv')\n",
    "        #Add transforms to the data which will allow it to be used in future models.\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            #assigning labels as features for the final dataset which will be used to train the model\n",
    "            labels = self.df.loc[index,'labels']\n",
    "            image_id = self.df.loc[index,'image_id']\n",
    "            emotions = []\n",
    "            for i in os.listdir('/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training'):\n",
    "                 file_path = f'/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/{i}'\n",
    "                 for i in file_path:\n",
    "                      with Image.open(f'{file_path}/{image_id}') as img:\n",
    "                           img.load()\n",
    "                      features = self.transform(img)\n",
    "                      return features,labels\n",
    "    \n",
    "    def __len__(self):    \n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "std evaluated to zero after conversion to torch.float32, leading to division by zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[39m=\u001b[39m Images()\n\u001b[0;32m----> 2\u001b[0m dataset[\u001b[39m242\u001b[39;49m]\n",
      "Cell \u001b[0;32mIn[43], line 32\u001b[0m, in \u001b[0;36mImages.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m Image\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mimage_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m img:\n\u001b[1;32m     31\u001b[0m      img\u001b[39m.\u001b[39mload()\n\u001b[0;32m---> 32\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m features,labels\n",
      "File \u001b[0;32m~/miniforge3/envs/scklearn/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniforge3/envs/scklearn/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/scklearn/lib/python3.11/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniforge3/envs/scklearn/lib/python3.11/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniforge3/envs/scklearn/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py:923\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    921\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(std, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m (std \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many():\n\u001b[0;32m--> 923\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstd evaluated to zero after conversion to \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m, leading to division by zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    924\u001b[0m \u001b[39mif\u001b[39;00m mean\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    925\u001b[0m     mean \u001b[39m=\u001b[39m mean\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: std evaluated to zero after conversion to torch.float32, leading to division by zero."
     ]
    }
   ],
   "source": [
    "dataset = Images()\n",
    "dataset[242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/Happy\n",
      "/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/.DS_Store\n",
      "/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/Sad\n",
      "/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/Fear\n",
      "/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/Neutral\n",
      "/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/Angry\n",
      "/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/Suprise\n"
     ]
    }
   ],
   "source": [
    "      for i in os.listdir('/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training'):\n",
    "                 file_path = f'/Users/rishanrahman/Desktop/facial_emotional_recognition/Images/Training/Training/{i}'\n",
    "                 print(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
